### 비선형적이란?
- 입력과 출력 사이의 관계까 직선(일차식)으로 표현되지 않는다는 뜻 -> 복잡한 패턴을 만들기 위함

<br>

### 초기 CNN이 Conv → BN → ReLU 순서로 이루어진 이유?
1. 먼저 특징을 만들어야 한다 → Conv (Weight)
2. 그런데 Conv 결과는 분포가 엉망이다
    - 평균이 계속 바뀌고
    - 분산이 층마다 달라지고
    - 학습이 불안정해진다 → 그래서 BN이 필요해졌다
3. 그런데 BN 뒤에는 비선형성이 있어야 표현력이 생긴다 → ReLU

<br>

### ResNet pre-activation에서의 순서가 초기의 CNN과 과정이 다른 이유는?
#### ```BN → ReLU → Weight```
1. 위와 같은 순서를 쓰는 이유는 "가중치가 가장 깨끗한 상태의 입력을 보게 하자"는 철학 때문
2. ResNet pre-activation의 핵심 목표
    1. Shortcut은 완전한 identity로 둔다
    2. Residual 경로에서는 계산이 가장 안정적인 상태에서 이루어지게 한다
3. **의문** → "왜 굳이 정리가 안 된 신호를 먼저 Conv에 넣는가? 정리부터 하고, 의미 있는 부분만 남긴 다음에 Conv를 통과시키면 되지 않은가?
4. 이러한 의문에서 출발하여 ```BN → ReLU → Conv```(2번 반복, 마지막에는 shortcut(x)와 더함) 순서로 개발됨
5. 즉, ```정리 → 선택 → 변형```
6. 뒤에는 더이상 ReLU를 붙이지 않음 → shortcut을 더한 뒤에 ReLU를 붙이면 shortcut 경로가 다시 잘리고 identity가 깨지기 때문임

<img width="347" height="402" alt="image" src="https://github.com/user-attachments/assets/8bdb7843-4ba8-463a-a331-47f66468a0b0" />

<br>

### DenseNet에서 Connection 개수 세기
1. 기본 식은 ```L(L + 1)/2```
2. connection 이라는 건, DenseNet에서는 각 층이 자기 뒤에 있는 모든 층과 직접 연결된다는 뜻임
3. 그래서 레이어가 6개 라면 connection이 연결되는 점은 7개(1 + 6)이 되는 것임

<br>

### Growth Rate?
1. Dense Block 내에서 레이어 하나당 추가되는 채널 수
2. Block이 끝나면 Transition Layer로 가서 이걸 다시 압축한다.

<br>

### bottleneck 레이어, transition 레이어, composite function
1. Composite Function
   - 한 레이어의 기본 연산 묶음
   - ```BN → ReLU → Conv``` : 이 세 개를 합쳐서 하나의 함수로 봄
   - 즉, ```정규화 → 비선형 → 합성곱```을 하나의 레이어 동작으로 묶은 것
2. Bottleneck Layer
   - Dense Block 내 각 레이어의 특정 형태의 내부 구조를 의미함
   - 구조 : ```[BN → ReLU → Conv(1×1)] → [BN → ReLU → Conv(3×3)]```
   - Conv(1X1)
       - 채널 수를 줄이는 압축 단계
       - 연산량 감소
   - Conv(3X3)
       - 실제 공간적 특징을 뽑는 단계
   - 좁은 통로 (1x1 Conv)를 한 번 통과시킨 후, 뒤의 큰 연산(3x3 Conv)을 가볍게 만든다는 의미임
3. Transition Layer
   - Dense Block과 Dense Block 사이에 들어가는 정리용 레이어
   - 구조는 보통: ```BN → Conv(1X1) → Avg Pooling```
   - 위 논문에서는 compression factor을 θ = 0.5으로 설정하였음

