##Introduction
###1-2문단
1. 지도 학습의 한계와 무감독 학습의 중요성
   - 가공되지 않은 텍스트로부터 효과적으로 학습하는 능력이 매우 중요
   - 시간과 비용이 많이 드는 레이블 비용을 대체할 대안이됨
   - 충분한 지도 학습 데이터가 있는 경우에도 무감독 방식은 학습 성능을 크게 향상 시킬 수 있음
   - 그 대표적인 예가 사전 학습된 워드 임베딩의 활용
2. 단어 수준 이상의 정보 활용 시 직면하는 두 가지 어려움
   - 최적화 목적 함수(Optimization Objective)의 불확실성
   - 전이 방식의 표준 부재
  
3. 준지도 학습(Semi-superviseed) 접근법 제안
   - 이 논문은 무감독 사전 학습(Unsupervised Pre-training)과 지도 미세 조정(Supervised Fine-tuning)을 결합한 준지도 학습 접근법을 탐구함
   - 목표는 다양한 과제에 적은 수정만으로도 전이될 수 있는 보편적인 표현(Universal Representation)을 학습하는 것
   - 학습방법
4. 모델 구조(Transformer) 구조를 채택함
   - 트랜스포머 : 장기 의존성(Long-term dependencies)을 처리하는 데 더 구조화된 메모리 제공
5. 실험 결과 및 성과
   - 4가지 유형의 과제에서 모델 평가
   - 12개 과제 중 9개에서 최첨단(SOTA) 기록을 경신함
   - 제로샷 행동 분석 결과, 모델이 사전 학습만으로도 하위 과제 수행에 유용한 언어적 지식을 습득한다는 사실 입증


### 3.1 Unsupervised pre-training
#### 학습 목표 및 목적함수
- 목표 : 레이블이 없는 대규모 텍스트 코퍼스로부터 고성능 언어 모델을 학습하는 것
- 방법: 표준 언어 모델링 목적 함수를 사용하여 다음의 Likelihood를 최대화
  - 이전 단어들을 보고 현재 단어가 나타날 확율을 높이도록 모델의 매개변수를 학습
  - 학습에는 확률적 경사 하강법(SGD)가 사용

#### 모델 구조: 트랜스포머 디코더 (Transformer Decoder)
- 트랜스포머의 다층 디코더 구조를 사용함
- 작동원리
  - 멀티 헤드 셀프 어텐션 연산을 적용
  - 포지션 와이즈 피드포워드 층 통과
 
#### 학습 데이터 및 설정
- 데이터셋 : 긴 연속 텍스트로 구성된 BooksCorpus 사용 (장기 의존성) 학습에 매우 유리
- 주요사양
  - 12개 레이어의 디코더 전용

### 3.2 Supervised fine-tunning
지도 미세 조정(Supervised Fine-tuning) 과정을 설명함

1. 기본 구조 및 과정
   - 데이터 구성 : 각 데이터는 토큰 시퀀스와 이에 해당하는 정답 레이블로 이루어져 있음
   - 모델 적용 : 트랜스포머 모델 통과
   - 출력층 추가 : 결과값을 선형 출력 레이어에 입력하여 정답 y를 예측
  
2. 목적 함수
   - 미세 조정 단계에서 최대화하고자 하는 기본 목적 함수
   - 모델이 주어진 입력에 대해 정확한 레이블을 맞출 확률을 극대화하도록 학습

3. 보조 목적 함수의 활용
   - 미세 조정 시 지도 학습 목적에 언어 모델링을 보조 목적 함수로 포함하는 것이 더 효과적임을 발견
   - 보조 목적 함수를 사용하는 이유
     - 일반화 개선 : 지도 학습 모델이 새로운 데이터에서도 잘 작동하도록 도움
     - 학습 수렴 가속화: 모델이 더 빠르게 최적의 상태에 도달하게 함
4. 특징
   - 최소한의 구조 변경
